{"cells":[{"source":["# This notebook was prepared by Taras Semenchenko"],"cell_type":"markdown","metadata":{}},{"metadata":{"id":"egZfFztcC8QW","trusted":true},"cell_type":"code","source":["%matplotlib inline\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","\n","import numpy as np\n","import pandas as pd\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"],"execution_count":1,"outputs":[]},{"metadata":{"scrolled":true,"id":"loswk9qqC8QX","outputId":"c7a376a8-918d-4529-f378-35650318f71c","trusted":true},"cell_type":"code","source":["df_train = pd.read_csv('../input/yelp-reviews-dataset/train.csv', header=None)\n","df_test = pd.read_csv('../input/yelp-reviews-dataset/test.csv', header=None)\n","df_train.columns = ['rating', 'review']\n","df_test.columns = ['rating', 'review']\n","df_train.head()"],"execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"   rating                                             review\n0       5  dr. goldberg offers everything i look for in a...\n1       2  Unfortunately, the frustration of being Dr. Go...\n2       4  Been going to Dr. Goldberg for over 10 years. ...\n3       4  Got a letter in the mail last week that said D...\n4       1  I don't know what Dr. Goldberg was like before...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rating</th>\n      <th>review</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5</td>\n      <td>dr. goldberg offers everything i look for in a...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Unfortunately, the frustration of being Dr. Go...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>Been going to Dr. Goldberg for over 10 years. ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>Got a letter in the mail last week that said D...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>I don't know what Dr. Goldberg was like before...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"id":"0vI_Dydf0cQd","outputId":"6b6f6166-733c-4a57-b2d6-6e91e4c7f6e4","trusted":true},"cell_type":"code","source":["df_train['review'][4]"],"execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"\"I don't know what Dr. Goldberg was like before  moving to Arizona, but let me tell you, STAY AWAY from this doctor and this office. I was going to Dr. Johnson before he left and Goldberg took over when Johnson left. He is not a caring doctor. He is only interested in the co-pay and having you come in for medication refills every month. He will not give refills and could less about patients's financial situations. Trying to get your 90 days mail away pharmacy prescriptions through this guy is a joke. And to make matters even worse, his office staff is incompetent. 90% of the time when you call the office, they'll put you through to a voice mail, that NO ONE ever answers or returns your call. Both my adult children and husband have decided to leave this practice after experiencing such frustration. The entire office has an attitude like they are doing you a favor. Give me a break! Stay away from this doc and the practice. You deserve better and they will not be there when you really need them. I have never felt compelled to write a bad review about anyone until I met this pathetic excuse for a doctor who is all about the money.\""},"metadata":{}}]},{"metadata":{"scrolled":true,"id":"fo3CWkFIC8QY","trusted":true},"cell_type":"code","source":["import re\n","\n","def clear_data(sent):\n","    clear_words = []\n","    for word in sent.lower().split():\n","        cleaned = re.findall('\\w+', word)\n","        if len(cleaned) > 0:\n","            clear_words += [cleaned[0]]\n","    return clear_words[:100]\n","\n","training_data = df_train.copy()\n","training_data['review'] = training_data['review'].apply(clear_data)\n","\n","\n","word2idx = {}\n","frequency = {}\n","for label, sent in training_data.values.tolist():\n","    for word in sent:\n","        if word not in word2idx:\n","            word2idx[word] = len(word2idx)\n","            frequency[word] = 1\n","        else:\n","            frequency[word] += 1\n","\n","label2idx = {1: 0, \n","             2: 1, \n","             3: 2,\n","             4: 3, \n","             5: 4\n","}"],"execution_count":4,"outputs":[]},{"metadata":{"id":"AxtE3Qacd7-M","outputId":"87cbc141-ed43-46da-c689-2b68ddc03561","trusted":true},"cell_type":"code","source":["sorted_frequency = sorted(list(frequency.items()), key=lambda x: x[1], reverse=True)\n","sorted_frequency[:5]"],"execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"[('the', 2497740),\n ('and', 1611212),\n ('i', 1495814),\n ('a', 1318494),\n ('to', 1187485)]"},"metadata":{}}]},{"metadata":{"id":"uHqWbDr4goYP","trusted":true},"cell_type":"code","source":["dictionary = sorted_frequency[:50000]\n","used_words = {word for word, count in dictionary}"],"execution_count":6,"outputs":[]},{"metadata":{"id":"0Bg3zZFrC8QZ","outputId":"224e5b54-a089-4ba0-f4d7-cc738d6afe74","trusted":true},"cell_type":"code","source":["training_data.head()"],"execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"   rating                                             review\n0       5  [dr, goldberg, offers, everything, i, look, fo...\n1       2  [unfortunately, the, frustration, of, being, d...\n2       4  [been, going, to, dr, goldberg, for, over, 10,...\n3       4  [got, a, letter, in, the, mail, last, week, th...\n4       1  [i, don, know, what, dr, goldberg, was, like, ...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rating</th>\n      <th>review</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5</td>\n      <td>[dr, goldberg, offers, everything, i, look, fo...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>[unfortunately, the, frustration, of, being, d...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>[been, going, to, dr, goldberg, for, over, 10,...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>[got, a, letter, in, the, mail, last, week, th...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>[i, don, know, what, dr, goldberg, was, like, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"id":"IFJOGDieC8Qa","trusted":true},"cell_type":"code","source":["import numpy as np\n","\n","# a helper function for converting a sequence of words to a Tensor of numerical values\n","# will be used later in training\n","def prepare_sequence(seq, to_idx):\n","    '''This function takes in a sequence of words and returns a \n","    corresponding Tensor of numerical values (indices for each word).'''\n","    idxs = [to_idx[w] if w in used_words else 99999 for w in seq]\n","    idxs = np.array(idxs)\n","    return torch.from_numpy(idxs)"],"execution_count":8,"outputs":[]},{"metadata":{"id":"5Y-YsyQ9C8Qa","outputId":"eb8c8304-65a7-479d-94c5-f7d515288e5a","trusted":true},"cell_type":"code","source":["# check out what prepare_sequence does for one of our training sentences:\n","example_input = prepare_sequence(clear_data(\"The dog answers the phone\"), word2idx)\n","print(example_input)"],"execution_count":9,"outputs":[{"output_type":"stream","text":"tensor([  70, 2310,   90,   70,   91])\n","name":"stdout"}]},{"metadata":{"id":"C6zUHMMHwrRc","outputId":"06d1c11e-c363-487c-eed5-ffcee94e5a20","trusted":true},"cell_type":"code","source":["df_train['cleaned'] = df_train['review'].apply(lambda x: prepare_sequence(clear_data(x), word2idx)[:100])\n","df_test['cleaned'] = df_test['review'].apply(lambda x: prepare_sequence(clear_data(x), word2idx)[:100])\n","df_train['cleaned']"],"execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"0         [tensor(0), tensor(1), tensor(2), tensor(3), t...\n1         [tensor(69), tensor(70), tensor(71), tensor(62...\n2         [tensor(123), tensor(124), tensor(15), tensor(...\n3         [tensor(164), tensor(8), tensor(165), tensor(7...\n4         [tensor(4), tensor(110), tensor(190), tensor(5...\n                                ...                        \n649995    [tensor(4), tensor(75), tensor(8), tensor(2666...\n649996    [tensor(91), tensor(4048), tensor(20), tensor(...\n649997    [tensor(419), tensor(183), tensor(136), tensor...\n649998    [tensor(4), tensor(128), tensor(254), tensor(1...\n649999    [tensor(4), tensor(34), tensor(123), tensor(18...\nName: cleaned, Length: 650000, dtype: object"},"metadata":{}}]},{"metadata":{"id":"Np8KxuOdyELe","outputId":"1434ae51-875e-4b5e-fff8-568e31ebd0af","trusted":true},"cell_type":"code","source":["padded_sequence_train = torch.nn.utils.rnn.pad_sequence(df_train['cleaned'], batch_first=True)\n","padded_sequence_test = torch.nn.utils.rnn.pad_sequence(df_test['cleaned'], batch_first=True)\n","padded_sequence_train"],"execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"tensor([[   0,    1,    2,  ...,    0,    0,    0],\n        [  69,   70,   71,  ...,   70,  122,   85],\n        [ 123,  124,   15,  ...,    0,    0,    0],\n        ...,\n        [ 419,  183,  136,  ...,    0,    0,    0],\n        [   4,  128,  254,  ...,    0,    0,    0],\n        [   4,   34,  123,  ...,   34,   15, 1064]])"},"metadata":{}}]},{"metadata":{"id":"Hx8Hh7FPC8Qb","trusted":true},"cell_type":"code","source":["import torch.nn.functional as F\n","\n","class LSTMTagger(nn.Module):\n","    \n","    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n","        ''' Initialize the layers of this model.'''\n","        super(LSTMTagger, self).__init__()\n","        \n","        self.hidden_dim = hidden_dim\n","\n","        # embedding layer that turns words into a vector of a specified size\n","        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n","        # the LSTM takes embedded word vectors (of a specified size) as inputs \n","        # and outputs hidden states of size hidden_dim\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n","\n","        self.c1 = nn.Conv1d(100, 100, 5, padding=2)\n","        # self.p1 = nn.MaxPool1d(2)\n","\n","        # the linear layer that maps the hidden state output dimension \n","        # to the number of tags we want as output, tagset_size (in this case this is 3 tags)\n","        \n","        # self.dence1 = nn.Linear(hidden_dim * 100 * 2, tagset_size)\n","        self.dence1 = nn.Linear(hidden_dim * 100 * 2, 32)\n","        self.dence2 = nn.Linear(32, tagset_size)\n","\n","        # self.hidden2tag = nn.Linear(hidden_dim * 100 * 2, tagset_size)\n","        \n","        # initialize the hidden state (see code below)\n","        self.hidden = self.init_hidden()\n","\n","        \n","    def init_hidden(self):\n","        ''' At the start of training, we need to initialize a hidden state;\n","           there will be none because the hidden state is formed based on perviously seen data.\n","           So, this function defines a hidden state with all zeroes and of a specified size.'''\n","        # The axes dimensions are (n_layers, batch_size, hidden_dim)\n","        return (torch.zeros(2, 100, self.hidden_dim).to(device),\n","                torch.zeros(2, 100, self.hidden_dim).to(device))\n","\n","    def forward(self, x):\n","        ''' Define the feedforward behavior of the model.'''\n","        # create embedded word vectors for each word in a sentence\n","        x = self.word_embeddings(x)\n","        # get the output and hidden state by passing the lstm over our word embeddings\n","        # the lstm takes in our embeddings and hiddent state\n","        # lstm_out, self.hidden = self.lstm(embeds.view(len(sentence), 1, -1), self.hidden)\n","        x, self.hidden = self.lstm(x, self.hidden)\n","        # lstm_out, self.hidden = self.lstm2(lstm_out, self.hidden)\n","        \n","        #print(lstm_out.shape)\n","\n","        # print(lstm_out.shape)\n","        \n","        \n","        x = F.relu(self.c1(x))\n","        # print(conv_out.shape)\n","        # conv_out = self.p1(conv_out)\n","\n","        # print(conv_out.shape)\n","\n","        # get the scores for the most likely tag for a word\n","        x = self.dence1(x.view(len(x), -1))\n","        x = self.dence2(x)\n","        logits = F.log_softmax(x, dim=1)\n","        \n","        return logits"],"execution_count":12,"outputs":[]},{"metadata":{"id":"ZkeZHaP2dTe2","trusted":true},"cell_type":"code","source":["import torch.utils.data as data_utils\n","\n","batch_size = 32\n","\n","labels_train = torch.from_numpy(df_train['rating'].to_numpy() - 1)\n","labels_test = torch.from_numpy(df_test['rating'].to_numpy() - 1)\n","\n","train = data_utils.TensorDataset(padded_sequence_train, labels_train)\n","test = data_utils.TensorDataset(padded_sequence_test, labels_test)\n","\n","train_loader = data_utils.DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=4)\n","test_loader = data_utils.DataLoader(test, batch_size=batch_size, shuffle=True, num_workers=4)"],"execution_count":14,"outputs":[]},{"metadata":{"id":"wMhVITbdC8Qc","trusted":true},"cell_type":"code","source":["# the embedding dimension defines the size of our word vectors\n","# for our simple vocabulary and training set, we will keep these small\n","EMBEDDING_DIM = 100\n","HIDDEN_DIM = 100\n","NUM_CLASSES = 5\n","BATCH_SIZE = 32\n","\n","# instantiate our model\n","model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word2idx), NUM_CLASSES).to(device)\n","\n","# define our loss and optimizer\n","# loss_function = nn.NLLLoss()\n","# loss_function = torch.nn.CrossEntropyLoss().to(device)\n","loss_function = torch.nn.NLLLoss().to(device)\n","# optimizer = optim.SGD(model.parameters(), lr=1)\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","lrscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer)"],"execution_count":15,"outputs":[]},{"metadata":{"id":"J2YO9iDoC8Qe","outputId":"05f0a56d-c87b-4130-ae21-b10e7dc7f33e","trusted":true},"cell_type":"code","source":["# normally these epochs take a lot longer \n","# but with our toy data (only 3 sentences), we can do many epochs in a short time\n","\n","from tqdm.notebook import tqdm\n","from torch.utils.data import DataLoader\n","\n","n_epochs = 5\n","\n","for epoch in range(n_epochs):\n","\n","    model.train()\n","\n","    train_losses = []\n","    train_accs = []\n","    \n","    iters = 0\n","    \n","    for index, (sentence, tag) in tqdm(enumerate(train_loader), total=len(training_data) // BATCH_SIZE):\n","                        \n","        model.zero_grad()\n","\n","            \n","        sentence, tag = sentence.to(device), tag.to(device)\n","        \n","        model.hidden = model.init_hidden()\n","                \n","        tag_scores = model(sentence)\n","\n","        loss = loss_function(tag_scores, tag)\n","        train_losses += [loss.item()]\n","        loss.backward()\n","\n","        train_accs += [(tag_scores.argmax(1) == tag).sum().item() / batch_size]\n","\n","        optimizer.step()\n","\n","    print(\"Epoch: %d, acc: %1.5f, loss: %1.5f\" % (epoch, np.mean(train_accs), np.mean(train_losses)))\n","    "],"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=20312.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5942c0487f7645efbf8153a9abb111cf"}},"metadata":{}},{"output_type":"stream","text":"\nEpoch: 0, acc: 0.53928, loss: 1.05301\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=20312.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36b56849ce1b494ea8853e6cec8fd863"}},"metadata":{}},{"output_type":"stream","text":"\nEpoch: 1, acc: 0.58211, loss: 0.96140\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=20312.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"607afec7227e46af88602d889078a207"}},"metadata":{}},{"output_type":"stream","text":"\nEpoch: 2, acc: 0.59982, loss: 0.92222\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=20312.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c32b4d9a51244729c96e40035c7eef6"}},"metadata":{}},{"output_type":"stream","text":"\nEpoch: 3, acc: 0.61514, loss: 0.88866\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=20312.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c076c7be16b4fb19ec02cc931a8459f"}},"metadata":{}},{"output_type":"stream","text":"\nEpoch: 4, acc: 0.63213, loss: 0.85434\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":["padded_sequence_test = padded_sequence_test.to(device)\n","output = model(padded_sequence_test)\n","labels_test = torch.from_numpy(df_test['rating'].to_numpy() - 1).to(device)\n","print('Test accuracy:', (output.argmax(1) == labels_test).sum().item() / len(labels_test))"],"execution_count":17,"outputs":[{"output_type":"stream","text":"Test accuracy: 0.5587\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}